{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCK2qFevmaAH"
      },
      "outputs": [],
      "source": [
        "#pip install laion-clap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn0RWsiqmsx3",
        "outputId": "041a609b-a299-4bf1-e24e-43d31d4e86cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/oac466/anaconda3/envs/clap-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "import laion_clap\n",
        "\n",
        "# quantization\n",
        "def int16_to_float32(x):\n",
        "    return (x / 32767.0).astype(np.float32)\n",
        "\n",
        "\n",
        "def float32_to_int16(x):\n",
        "    x = np.clip(x, a_min=-1., a_max=1.)\n",
        "    return (x * 32767.).astype(np.int16)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ~/Downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/oac466/anaconda3/envs/clap-env/lib/python3.11/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1711403212643/work/aten/src/ATen/native/TensorShape.cpp:3550.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load the specified checkpoint /Users/oac466/Downloads/music_speech_audioset_epoch_15_esc_89.98.pt from users.\n",
            "Load Checkpoint...\n",
            "logit_scale_a \t Loaded\n",
            "logit_scale_t \t Loaded\n",
            "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
            "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
            "audio_branch.logmel_extractor.melW \t Loaded\n",
            "audio_branch.bn0.weight \t Loaded\n",
            "audio_branch.bn0.bias \t Loaded\n",
            "audio_branch.patch_embed.proj.weight \t Loaded\n",
            "audio_branch.patch_embed.proj.bias \t Loaded\n",
            "audio_branch.patch_embed.norm.weight \t Loaded\n",
            "audio_branch.patch_embed.norm.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
            "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
            "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
            "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
            "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
            "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
            "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
            "audio_branch.norm.weight \t Loaded\n",
            "audio_branch.norm.bias \t Loaded\n",
            "audio_branch.tscam_conv.weight \t Loaded\n",
            "audio_branch.tscam_conv.bias \t Loaded\n",
            "audio_branch.head.weight \t Loaded\n",
            "audio_branch.head.bias \t Loaded\n",
            "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
            "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
            "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
            "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
            "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
            "text_branch.pooler.dense.weight \t Loaded\n",
            "text_branch.pooler.dense.bias \t Loaded\n",
            "text_transform.sequential.0.weight \t Loaded\n",
            "text_transform.sequential.0.bias \t Loaded\n",
            "text_transform.sequential.3.weight \t Loaded\n",
            "text_transform.sequential.3.bias \t Loaded\n",
            "text_projection.0.weight \t Loaded\n",
            "text_projection.0.bias \t Loaded\n",
            "text_projection.2.weight \t Loaded\n",
            "text_projection.2.bias \t Loaded\n",
            "audio_transform.sequential.0.weight \t Loaded\n",
            "audio_transform.sequential.0.bias \t Loaded\n",
            "audio_transform.sequential.3.weight \t Loaded\n",
            "audio_transform.sequential.3.bias \t Loaded\n",
            "audio_projection.0.weight \t Loaded\n",
            "audio_projection.0.bias \t Loaded\n",
            "audio_projection.2.weight \t Loaded\n",
            "audio_projection.2.bias \t Loaded\n"
          ]
        }
      ],
      "source": [
        "# model = laion_clap.CLAP_Module(enable_fusion=False)\n",
        "# model.load_ckpt()\n",
        "\n",
        "model = laion_clap.CLAP_Module(enable_fusion=False, amodel= 'HTSAT-base')\n",
        "model.load_ckpt('/Users/oac466/Downloads/music_speech_audioset_epoch_15_esc_89.98.pt') # download the default pretrained checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tOTDm9ccnW-w"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from natsort import natsorted\n",
        "#from scipy.io import wavfile\n",
        "\n",
        "# Path to main folder containing all subfolders\n",
        "main_folder = \"data/mic1_trim_v2\"\n",
        "\n",
        "# First, get the subfolders in a natural-sorted list\n",
        "# (in case your subfolders also have numeric components in their names)\n",
        "subfolders = [\n",
        "    f for f in os.listdir(main_folder)\n",
        "    if os.path.isdir(os.path.join(main_folder, f))\n",
        "]\n",
        "subfolders = natsorted(subfolders)\n",
        "\n",
        "all_wavs = []  # to collect (filepath, sr, data) or similar\n",
        "\n",
        "for subfolder in subfolders:\n",
        "    subfolder_path = os.path.join(main_folder, subfolder)\n",
        "\n",
        "    # List .wav files in subfolder\n",
        "    wav_files = [\n",
        "        f for f in os.listdir(subfolder_path)\n",
        "        if f.lower().endswith(\".wav\")\n",
        "    ]\n",
        "    # Sort them in natural order\n",
        "    wav_files = natsorted(wav_files)\n",
        "\n",
        "    # Process each .wav file\n",
        "    for wav_file in wav_files:\n",
        "        wav_path = os.path.join(subfolder_path, wav_file)\n",
        "\n",
        "        # for example, using librosa (just as a placeholder)\n",
        "        # import librosa\n",
        "        # data, sr = librosa.load(wav_path, sr=None)\n",
        "\n",
        "        # or using scipy\n",
        "        data,sr = librosa.load(wav_path,sr=48000)\n",
        "\n",
        "        # collect or do something with the data\n",
        "        all_wavs.append((wav_path, sr, data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gJdXvha5nk_A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "meta_participant = pd.read_csv('data/meta_participant.csv')\n",
        "meta_audio = pd.read_csv('data/meta_audio.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PID</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>AGE</th>\n",
              "      <th>RACE/ETHNICITY</th>\n",
              "      <th>FOLDER SIZE (MB)</th>\n",
              "      <th>NUMBER OF FILES</th>\n",
              "      <th>TOTAL DURATION (SEC)</th>\n",
              "      <th>LC</th>\n",
              "      <th>LW</th>\n",
              "      <th>RC</th>\n",
              "      <th>RW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>p10085</td>\n",
              "      <td>Woman</td>\n",
              "      <td>21</td>\n",
              "      <td>Asian</td>\n",
              "      <td>9.6</td>\n",
              "      <td>119</td>\n",
              "      <td>300.45</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>p10555</td>\n",
              "      <td>Woman</td>\n",
              "      <td>19</td>\n",
              "      <td>Asian</td>\n",
              "      <td>13.8</td>\n",
              "      <td>160</td>\n",
              "      <td>431.22</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>p11085</td>\n",
              "      <td>Non-binary</td>\n",
              "      <td>18</td>\n",
              "      <td>Two or more races</td>\n",
              "      <td>13.4</td>\n",
              "      <td>142</td>\n",
              "      <td>419.28</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>p12385</td>\n",
              "      <td>Man</td>\n",
              "      <td>23</td>\n",
              "      <td>White</td>\n",
              "      <td>12.5</td>\n",
              "      <td>155</td>\n",
              "      <td>391.89</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>p12460</td>\n",
              "      <td>Man</td>\n",
              "      <td>20</td>\n",
              "      <td>Two or more races</td>\n",
              "      <td>12.9</td>\n",
              "      <td>168</td>\n",
              "      <td>403.02</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>p15965</td>\n",
              "      <td>Man</td>\n",
              "      <td>19</td>\n",
              "      <td>Asian</td>\n",
              "      <td>12.7</td>\n",
              "      <td>170</td>\n",
              "      <td>396.87</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>p18355</td>\n",
              "      <td>Woman</td>\n",
              "      <td>20</td>\n",
              "      <td>Asian</td>\n",
              "      <td>8.4</td>\n",
              "      <td>99</td>\n",
              "      <td>262.26</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>p18785</td>\n",
              "      <td>Woman</td>\n",
              "      <td>27</td>\n",
              "      <td>White</td>\n",
              "      <td>9.1</td>\n",
              "      <td>99</td>\n",
              "      <td>282.81</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>p20960</td>\n",
              "      <td>Man</td>\n",
              "      <td>18</td>\n",
              "      <td>White</td>\n",
              "      <td>13.6</td>\n",
              "      <td>161</td>\n",
              "      <td>423.75</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>p21425</td>\n",
              "      <td>Man</td>\n",
              "      <td>33</td>\n",
              "      <td>Asian</td>\n",
              "      <td>11.7</td>\n",
              "      <td>117</td>\n",
              "      <td>365.25</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>p22250</td>\n",
              "      <td>Woman</td>\n",
              "      <td>19</td>\n",
              "      <td>Asian</td>\n",
              "      <td>13.3</td>\n",
              "      <td>156</td>\n",
              "      <td>416.91</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>p23900</td>\n",
              "      <td>Woman</td>\n",
              "      <td>20</td>\n",
              "      <td>Asian</td>\n",
              "      <td>13.6</td>\n",
              "      <td>149</td>\n",
              "      <td>424.59</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>p28030</td>\n",
              "      <td>Woman</td>\n",
              "      <td>19</td>\n",
              "      <td>White</td>\n",
              "      <td>13.6</td>\n",
              "      <td>159</td>\n",
              "      <td>425.55</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>p31855</td>\n",
              "      <td>Woman</td>\n",
              "      <td>25</td>\n",
              "      <td>White</td>\n",
              "      <td>7.7</td>\n",
              "      <td>78</td>\n",
              "      <td>239.49</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>p37540</td>\n",
              "      <td>Woman</td>\n",
              "      <td>19</td>\n",
              "      <td>Asian</td>\n",
              "      <td>13.1</td>\n",
              "      <td>145</td>\n",
              "      <td>409.80</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>p38510</td>\n",
              "      <td>Man</td>\n",
              "      <td>25</td>\n",
              "      <td>Asian</td>\n",
              "      <td>9.7</td>\n",
              "      <td>111</td>\n",
              "      <td>301.47</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>p40505</td>\n",
              "      <td>Woman</td>\n",
              "      <td>19</td>\n",
              "      <td>Asian</td>\n",
              "      <td>12.4</td>\n",
              "      <td>151</td>\n",
              "      <td>387.15</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>p47080</td>\n",
              "      <td>Woman</td>\n",
              "      <td>23</td>\n",
              "      <td>White</td>\n",
              "      <td>12.0</td>\n",
              "      <td>158</td>\n",
              "      <td>374.94</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>p50580</td>\n",
              "      <td>Woman</td>\n",
              "      <td>18</td>\n",
              "      <td>White</td>\n",
              "      <td>12.9</td>\n",
              "      <td>163</td>\n",
              "      <td>403.17</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>p50635</td>\n",
              "      <td>Man</td>\n",
              "      <td>19</td>\n",
              "      <td>Asian</td>\n",
              "      <td>8.1</td>\n",
              "      <td>85</td>\n",
              "      <td>254.25</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>p52610</td>\n",
              "      <td>Man</td>\n",
              "      <td>18</td>\n",
              "      <td>White</td>\n",
              "      <td>12.9</td>\n",
              "      <td>157</td>\n",
              "      <td>404.13</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>p58845</td>\n",
              "      <td>Woman</td>\n",
              "      <td>19</td>\n",
              "      <td>Asian</td>\n",
              "      <td>11.7</td>\n",
              "      <td>144</td>\n",
              "      <td>364.14</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>p59330</td>\n",
              "      <td>Man</td>\n",
              "      <td>29</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>7.8</td>\n",
              "      <td>96</td>\n",
              "      <td>244.74</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>p59520</td>\n",
              "      <td>Woman</td>\n",
              "      <td>23</td>\n",
              "      <td>White</td>\n",
              "      <td>13.6</td>\n",
              "      <td>157</td>\n",
              "      <td>423.39</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>p60145</td>\n",
              "      <td>Woman</td>\n",
              "      <td>18</td>\n",
              "      <td>Asian</td>\n",
              "      <td>12.0</td>\n",
              "      <td>165</td>\n",
              "      <td>374.67</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>p61160</td>\n",
              "      <td>Man</td>\n",
              "      <td>18</td>\n",
              "      <td>Asian</td>\n",
              "      <td>10.1</td>\n",
              "      <td>108</td>\n",
              "      <td>314.79</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>p61395</td>\n",
              "      <td>Woman</td>\n",
              "      <td>19</td>\n",
              "      <td>White</td>\n",
              "      <td>12.8</td>\n",
              "      <td>164</td>\n",
              "      <td>399.57</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>p62650</td>\n",
              "      <td>Woman</td>\n",
              "      <td>33</td>\n",
              "      <td>Asian</td>\n",
              "      <td>10.9</td>\n",
              "      <td>111</td>\n",
              "      <td>325.62</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>p64560</td>\n",
              "      <td>Man</td>\n",
              "      <td>18</td>\n",
              "      <td>Asian</td>\n",
              "      <td>13.0</td>\n",
              "      <td>152</td>\n",
              "      <td>406.35</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>p65330</td>\n",
              "      <td>Non-binary</td>\n",
              "      <td>19</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>14.1</td>\n",
              "      <td>149</td>\n",
              "      <td>441.03</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>p65870</td>\n",
              "      <td>Man</td>\n",
              "      <td>22</td>\n",
              "      <td>Asian</td>\n",
              "      <td>13.5</td>\n",
              "      <td>135</td>\n",
              "      <td>421.92</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>p66385</td>\n",
              "      <td>Man</td>\n",
              "      <td>18</td>\n",
              "      <td>Asian</td>\n",
              "      <td>8.6</td>\n",
              "      <td>115</td>\n",
              "      <td>268.14</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>p68215</td>\n",
              "      <td>Woman</td>\n",
              "      <td>18</td>\n",
              "      <td>Asian</td>\n",
              "      <td>4.5</td>\n",
              "      <td>56</td>\n",
              "      <td>139.35</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>p68340</td>\n",
              "      <td>Woman</td>\n",
              "      <td>18</td>\n",
              "      <td>Asian</td>\n",
              "      <td>12.9</td>\n",
              "      <td>166</td>\n",
              "      <td>401.94</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>p68625</td>\n",
              "      <td>Non-binary</td>\n",
              "      <td>21</td>\n",
              "      <td>Two or more races</td>\n",
              "      <td>12.9</td>\n",
              "      <td>164</td>\n",
              "      <td>404.37</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>p68870</td>\n",
              "      <td>Man</td>\n",
              "      <td>32</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>12.5</td>\n",
              "      <td>146</td>\n",
              "      <td>390.51</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>p71740</td>\n",
              "      <td>Woman</td>\n",
              "      <td>19</td>\n",
              "      <td>Asian</td>\n",
              "      <td>10.3</td>\n",
              "      <td>126</td>\n",
              "      <td>322.59</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>p72315</td>\n",
              "      <td>Man</td>\n",
              "      <td>21</td>\n",
              "      <td>White</td>\n",
              "      <td>12.5</td>\n",
              "      <td>145</td>\n",
              "      <td>391.26</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>p74405</td>\n",
              "      <td>Woman</td>\n",
              "      <td>20</td>\n",
              "      <td>Two or more races</td>\n",
              "      <td>13.0</td>\n",
              "      <td>157</td>\n",
              "      <td>407.52</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>p77220</td>\n",
              "      <td>Man</td>\n",
              "      <td>28</td>\n",
              "      <td>Asian</td>\n",
              "      <td>8.8</td>\n",
              "      <td>101</td>\n",
              "      <td>273.30</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>p77560</td>\n",
              "      <td>Man</td>\n",
              "      <td>30</td>\n",
              "      <td>Asian</td>\n",
              "      <td>13.1</td>\n",
              "      <td>144</td>\n",
              "      <td>407.82</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>p79550</td>\n",
              "      <td>Woman</td>\n",
              "      <td>21</td>\n",
              "      <td>White</td>\n",
              "      <td>13.8</td>\n",
              "      <td>142</td>\n",
              "      <td>430.53</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>p79665</td>\n",
              "      <td>Man</td>\n",
              "      <td>18</td>\n",
              "      <td>White</td>\n",
              "      <td>12.6</td>\n",
              "      <td>142</td>\n",
              "      <td>394.92</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>p80330</td>\n",
              "      <td>Male</td>\n",
              "      <td>19</td>\n",
              "      <td>Asian</td>\n",
              "      <td>12.3</td>\n",
              "      <td>163</td>\n",
              "      <td>383.49</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>p86910</td>\n",
              "      <td>Woman</td>\n",
              "      <td>25</td>\n",
              "      <td>White</td>\n",
              "      <td>11.0</td>\n",
              "      <td>121</td>\n",
              "      <td>344.16</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>p88575</td>\n",
              "      <td>Man</td>\n",
              "      <td>19</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>12.9</td>\n",
              "      <td>168</td>\n",
              "      <td>402.36</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>p88775</td>\n",
              "      <td>Woman</td>\n",
              "      <td>20</td>\n",
              "      <td>Black or African American</td>\n",
              "      <td>7.7</td>\n",
              "      <td>92</td>\n",
              "      <td>241.71</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>p91315</td>\n",
              "      <td>Woman</td>\n",
              "      <td>19</td>\n",
              "      <td>Asian</td>\n",
              "      <td>13.7</td>\n",
              "      <td>145</td>\n",
              "      <td>429.03</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>p93975</td>\n",
              "      <td>Woman</td>\n",
              "      <td>20</td>\n",
              "      <td>Asian</td>\n",
              "      <td>12.9</td>\n",
              "      <td>163</td>\n",
              "      <td>403.38</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>p94215</td>\n",
              "      <td>Man</td>\n",
              "      <td>24</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>12.8</td>\n",
              "      <td>160</td>\n",
              "      <td>398.76</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>p97630</td>\n",
              "      <td>Man</td>\n",
              "      <td>18</td>\n",
              "      <td>Asian</td>\n",
              "      <td>12.5</td>\n",
              "      <td>145</td>\n",
              "      <td>391.26</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       PID      GENDER  AGE             RACE/ETHNICITY  FOLDER SIZE (MB)  \\\n",
              "0   p10085       Woman   21                      Asian               9.6   \n",
              "1   p10555       Woman   19                      Asian              13.8   \n",
              "2   p11085  Non-binary   18          Two or more races              13.4   \n",
              "3   p12385         Man   23                      White              12.5   \n",
              "4   p12460         Man   20          Two or more races              12.9   \n",
              "5   p15965         Man   19                      Asian              12.7   \n",
              "6   p18355       Woman   20                      Asian               8.4   \n",
              "7   p18785       Woman   27                      White               9.1   \n",
              "8   p20960         Man   18                      White              13.6   \n",
              "9   p21425         Man   33                      Asian              11.7   \n",
              "10  p22250       Woman   19                      Asian              13.3   \n",
              "11  p23900       Woman   20                      Asian              13.6   \n",
              "12  p28030       Woman   19                      White              13.6   \n",
              "13  p31855       Woman   25                      White               7.7   \n",
              "14  p37540       Woman   19                      Asian              13.1   \n",
              "15  p38510         Man   25                      Asian               9.7   \n",
              "16  p40505       Woman   19                      Asian              12.4   \n",
              "17  p47080       Woman   23                      White              12.0   \n",
              "18  p50580       Woman   18                      White              12.9   \n",
              "19  p50635         Man   19                      Asian               8.1   \n",
              "20  p52610         Man   18                      White              12.9   \n",
              "21  p58845       Woman   19                      Asian              11.7   \n",
              "22  p59330         Man   29            Hispanic/Latino               7.8   \n",
              "23  p59520       Woman   23                      White              13.6   \n",
              "24  p60145       Woman   18                      Asian              12.0   \n",
              "25  p61160         Man   18                      Asian              10.1   \n",
              "26  p61395       Woman   19                      White              12.8   \n",
              "27  p62650       Woman   33                      Asian              10.9   \n",
              "28  p64560         Man   18                      Asian              13.0   \n",
              "29  p65330  Non-binary   19            Hispanic/Latino              14.1   \n",
              "30  p65870         Man   22                      Asian              13.5   \n",
              "31  p66385         Man   18                      Asian               8.6   \n",
              "32  p68215       Woman   18                      Asian               4.5   \n",
              "33  p68340       Woman   18                      Asian              12.9   \n",
              "34  p68625  Non-binary   21          Two or more races              12.9   \n",
              "35  p68870         Man   32            Hispanic/Latino              12.5   \n",
              "36  p71740       Woman   19                      Asian              10.3   \n",
              "37  p72315         Man   21                      White              12.5   \n",
              "38  p74405       Woman   20          Two or more races              13.0   \n",
              "39  p77220         Man   28                      Asian               8.8   \n",
              "40  p77560         Man   30                      Asian              13.1   \n",
              "41  p79550       Woman   21                      White              13.8   \n",
              "42  p79665         Man   18                      White              12.6   \n",
              "43  p80330        Male   19                      Asian              12.3   \n",
              "44  p86910       Woman   25                      White              11.0   \n",
              "45  p88575         Man   19            Hispanic/Latino              12.9   \n",
              "46  p88775       Woman   20  Black or African American               7.7   \n",
              "47  p91315       Woman   19                      Asian              13.7   \n",
              "48  p93975       Woman   20                      Asian              12.9   \n",
              "49  p94215         Man   24            Hispanic/Latino              12.8   \n",
              "50  p97630         Man   18                      Asian              12.5   \n",
              "\n",
              "    NUMBER OF FILES  TOTAL DURATION (SEC)  LC  LW  RC  RW  \n",
              "0               119                300.45   1   1   1   1  \n",
              "1               160                431.22   1   1   1   1  \n",
              "2               142                419.28   1   1   1   1  \n",
              "3               155                391.89   1   1   1   1  \n",
              "4               168                403.02   1   1   1   1  \n",
              "5               170                396.87   1   1   1   1  \n",
              "6                99                262.26   0   1   0   1  \n",
              "7                99                282.81   0   1   0   1  \n",
              "8               161                423.75   1   1   1   1  \n",
              "9               117                365.25   1   1   1   1  \n",
              "10              156                416.91   1   1   1   1  \n",
              "11              149                424.59   1   1   1   1  \n",
              "12              159                425.55   1   1   1   1  \n",
              "13               78                239.49   1   1   1   1  \n",
              "14              145                409.80   1   1   1   1  \n",
              "15              111                301.47   0   1   0   1  \n",
              "16              151                387.15   1   1   1   1  \n",
              "17              158                374.94   0   1   1   1  \n",
              "18              163                403.17   1   1   1   1  \n",
              "19               85                254.25   0   1   0   1  \n",
              "20              157                404.13   1   1   1   1  \n",
              "21              144                364.14   0   1   1   1  \n",
              "22               96                244.74   0   1   0   1  \n",
              "23              157                423.39   1   1   1   1  \n",
              "24              165                374.67   1   1   1   1  \n",
              "25              108                314.79   0   1   1   1  \n",
              "26              164                399.57   1   1   1   1  \n",
              "27              111                325.62   0   1   1   1  \n",
              "28              152                406.35   1   1   1   1  \n",
              "29              149                441.03   1   1   1   1  \n",
              "30              135                421.92   1   1   1   1  \n",
              "31              115                268.14   0   1   0   1  \n",
              "32               56                139.35   1   1   0   0  \n",
              "33              166                401.94   1   1   1   1  \n",
              "34              164                404.37   1   1   1   1  \n",
              "35              146                390.51   1   1   1   1  \n",
              "36              126                322.59   0   1   1   1  \n",
              "37              145                391.26   1   1   1   1  \n",
              "38              157                407.52   1   1   1   1  \n",
              "39              101                273.30   0   1   0   1  \n",
              "40              144                407.82   1   1   1   1  \n",
              "41              142                430.53   1   1   1   1  \n",
              "42              142                394.92   1   1   1   1  \n",
              "43              163                383.49   1   1   1   0  \n",
              "44              121                344.16   0   1   1   1  \n",
              "45              168                402.36   1   1   1   1  \n",
              "46               92                241.71   0   1   0   1  \n",
              "47              145                429.03   1   1   1   1  \n",
              "48              163                403.38   1   1   1   1  \n",
              "49              160                398.76   1   1   1   1  \n",
              "50              145                391.26   1   1   1   1  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "meta_participant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r_NMImAhnm5r"
      },
      "outputs": [],
      "source": [
        "filtered_indices = meta_audio[meta_audio['ACTION LABEL'].isin([0, 1, 2])].index\n",
        "# filtered_wavs = [all_wavs[i] for i in filtered_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyiC3B0JmyJB",
        "outputId": "3f0c43fb-0fe4-4ceb-fe77-407bbc9dae83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.01473512  0.01407791  0.0626817   0.08060931  0.08191844  0.0108817\n",
            "  -0.01646244 -0.03592062 -0.03780748  0.04159889 -0.02378965 -0.02779254\n",
            "  -0.09213071 -0.01054646  0.0902087  -0.01271668 -0.03600216  0.08788741\n",
            "   0.00368626 -0.05718187]]\n",
            "(1, 512)\n"
          ]
        }
      ],
      "source": [
        "# Get audio embeddings from audio data\n",
        "audio_data, _ = librosa.load('data/mic1_trim_v2/p10085/p10085.LC.1.161.wav', sr=48000) # sample rate should be 48000\n",
        "audio_data = audio_data.reshape(1, -1) # Make it (1,T) or (N,T)\n",
        "\n",
        "audio_embed = model.get_audio_embedding_from_data(x = audio_data, use_tensor=False)\n",
        "print(audio_embed[:,-20:])\n",
        "print(audio_embed.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOaksYtQpNxN",
        "outputId": "129f4feb-0091-4b1e-e3b6-13ba12b2d6f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7044/7044 [25:20<00:00,  4.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.01473512  0.01407791  0.0626817   0.08060931  0.08191844  0.0108817\n",
            "  -0.01646244 -0.03592062 -0.03780748  0.04159889 -0.02378965 -0.02779254\n",
            "  -0.09213071 -0.01054646  0.0902087  -0.01271668 -0.03600216  0.08788741\n",
            "   0.00368626 -0.05718187]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1, 512)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm import tqdm # Import the tqdm function from the tqdm module\n",
        "\n",
        "embeddings = []\n",
        "for i in tqdm(range(len(all_wavs))): # Now, you're calling the tqdm function\n",
        "  audio_data = all_wavs[i][2]\n",
        "  audio_data = audio_data.reshape(1, -1)\n",
        "  audio_embed = model.get_audio_embedding_from_data(x=audio_data, use_tensor=False)\n",
        "  embeddings.append(audio_embed)\n",
        "\n",
        "print(embeddings[0][:,-20:])\n",
        "embeddings[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                                                     NaN\n",
              "1                                                     NaN\n",
              "2                                                     NaN\n",
              "3                                                     NaN\n",
              "4                                                     NaN\n",
              "                              ...                        \n",
              "7039    no pain rating given so copied down rather tha...\n",
              "7040    click in middle; no pain rating given so copie...\n",
              "7041    no pain rating given so copied down rather tha...\n",
              "7042    first word \"Help\" cut out; slight click at end...\n",
              "7043    majority of sentence cut out (\"the pain I feel...\n",
              "Name: NOTES, Length: 7044, dtype: object"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "meta_audio['NOTES']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBa57RwJcwJY",
        "outputId": "7cb281ab-241c-4c4f-9a46-f991ade9bc52"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7044/7044 [07:59<00:00, 14.69it/s]\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m   text_embeddings.append(text_embed)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# print(text_embeddings)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mtext_embeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\n",
            "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm # Import the tqdm function from the tqdm module\n",
        "\n",
        "text_embeddings = []\n",
        "for i in tqdm(range(len(meta_audio))): # Now, you're calling the tqdm function\n",
        "  text_data = meta_audio['NOTES'][i]\n",
        "  if not isinstance(text_data, str):\n",
        "    text_data = ' '\n",
        "  text_embed = model.get_text_embedding(text_data)\n",
        "  text_embeddings.append(text_embed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7044, 1, 512)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# print(text_embeddings)\n",
        "np.array(text_embeddings).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbc_aIiVg32H",
        "outputId": "34114774-584a-4b41-9bea-9d3f41d279e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7044, 512)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_embeddings_array = np.array(text_embeddings).squeeze()\n",
        "text_embeddings_array.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Z1PBw7Z2inee"
      },
      "outputs": [],
      "source": [
        "np.save('Text Embeddings.npy', text_embeddings_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Pc7s7rd9TtNN"
      },
      "outputs": [],
      "source": [
        "# Convert embeddings to a NumPy array\n",
        "X = np.array(embeddings).squeeze() # Squeeze to remove extra dimensions if necessary\n",
        "X_filtered = X[filtered_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AuY-HgVWxs7x"
      },
      "outputs": [],
      "source": [
        "np.save('Audio Embeddings.npy', X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ8dsTI7cp6m",
        "outputId": "20a95ce3-22e2-4414-c740-5f7623ebc887"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7044, 512)\n"
          ]
        }
      ],
      "source": [
        "# prompt: load Audio Embeddings.npy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved embeddings\n",
        "audio_embeddings = np.load('Audio Embeddings.npy')\n",
        "\n",
        "# Now you can work with the loaded embeddings\n",
        "print(audio_embeddings.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP8BvWs5hP0O",
        "outputId": "c0056e97-ce3b-4301-aa9c-26e49832ddde"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7044, 1024)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_combined = np.concatenate((audio_embeddings, text_embeddings_array), axis=1)\n",
        "embeddings_combined.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "IHHR-gJ4qdZD"
      },
      "outputs": [],
      "source": [
        "participant_data= meta_participant.loc[meta_participant.index.repeat(meta_participant[\"NUMBER OF FILES\"])]\n",
        "\n",
        "# 3. Reset the index (optional, but usually helpful)\n",
        "participant_data.reset_index(drop=True, inplace=True)\n",
        "# Merge meta_audio and participant_data DataFrames\n",
        "merged_df = pd.merge(meta_audio, participant_data, left_index=True, right_index=True)\n",
        "# merged_df = merged_df[merged_df['ACTION LABEL'].isin([0, 1, 2])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RW</th>\n",
              "      <th>RC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7039</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7040</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7041</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7042</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7043</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7044 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      RW  RC\n",
              "0      1   1\n",
              "1      1   1\n",
              "2      1   1\n",
              "3      1   1\n",
              "4      1   1\n",
              "...   ..  ..\n",
              "7039   1   1\n",
              "7040   1   1\n",
              "7041   1   1\n",
              "7042   1   1\n",
              "7043   1   1\n",
              "\n",
              "[7044 rows x 2 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_df[['RW', 'RC']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "hXEAv_BuqocM"
      },
      "outputs": [],
      "source": [
        "tabular_data = merged_df.loc[:, ['GENDER', 'AGE', 'RACE/ETHNICITY', 'TOTAL DURATION (SEC)']]\n",
        "tabular_data = pd.get_dummies(tabular_data, columns=['GENDER', 'RACE/ETHNICITY'], dtype=np.float32)\n",
        "num_tab_features = tabular_data.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "K_DRNesxqmCA"
      },
      "outputs": [],
      "source": [
        "labels = merged_df.loc[:,'REVISED PAIN']\n",
        "labels= np.array([0 if x < 4 else 1 for x in labels]) # Labels: 0 or 1 for \"No Pain\"/\"Pain\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "4zFTBV43sLL_"
      },
      "outputs": [],
      "source": [
        "# prompt: fit a random forest classifier using embeddings as vairables and labels as target\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "fI6dZsYD6Ltd",
        "outputId": "5db125bb-ce21-4fec-c40c-5b376e308a38"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>TOTAL DURATION (SEC)</th>\n",
              "      <th>GENDER_Male</th>\n",
              "      <th>GENDER_Man</th>\n",
              "      <th>GENDER_Non-binary</th>\n",
              "      <th>GENDER_Woman</th>\n",
              "      <th>RACE/ETHNICITY_Asian</th>\n",
              "      <th>RACE/ETHNICITY_Black or African American</th>\n",
              "      <th>RACE/ETHNICITY_Hispanic/Latino</th>\n",
              "      <th>RACE/ETHNICITY_Two or more races</th>\n",
              "      <th>RACE/ETHNICITY_White</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.057114</td>\n",
              "      <td>0.059313</td>\n",
              "      <td>-0.060191</td>\n",
              "      <td>-0.018176</td>\n",
              "      <td>-0.084338</td>\n",
              "      <td>0.034319</td>\n",
              "      <td>-0.058333</td>\n",
              "      <td>0.018405</td>\n",
              "      <td>0.068168</td>\n",
              "      <td>-0.077160</td>\n",
              "      <td>...</td>\n",
              "      <td>300.45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.074543</td>\n",
              "      <td>0.059482</td>\n",
              "      <td>-0.023707</td>\n",
              "      <td>-0.031039</td>\n",
              "      <td>-0.080793</td>\n",
              "      <td>0.020725</td>\n",
              "      <td>-0.059566</td>\n",
              "      <td>0.012630</td>\n",
              "      <td>0.088856</td>\n",
              "      <td>-0.075533</td>\n",
              "      <td>...</td>\n",
              "      <td>300.45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.088873</td>\n",
              "      <td>0.066683</td>\n",
              "      <td>-0.039078</td>\n",
              "      <td>-0.013078</td>\n",
              "      <td>-0.081851</td>\n",
              "      <td>0.002385</td>\n",
              "      <td>-0.033466</td>\n",
              "      <td>0.038994</td>\n",
              "      <td>0.053495</td>\n",
              "      <td>-0.089920</td>\n",
              "      <td>...</td>\n",
              "      <td>300.45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.079781</td>\n",
              "      <td>0.052505</td>\n",
              "      <td>-0.031692</td>\n",
              "      <td>-0.014320</td>\n",
              "      <td>-0.075479</td>\n",
              "      <td>0.012609</td>\n",
              "      <td>-0.057462</td>\n",
              "      <td>-0.004710</td>\n",
              "      <td>0.082208</td>\n",
              "      <td>-0.084615</td>\n",
              "      <td>...</td>\n",
              "      <td>300.45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.067623</td>\n",
              "      <td>0.042367</td>\n",
              "      <td>-0.049939</td>\n",
              "      <td>-0.033383</td>\n",
              "      <td>-0.096456</td>\n",
              "      <td>0.037647</td>\n",
              "      <td>-0.042283</td>\n",
              "      <td>0.001679</td>\n",
              "      <td>0.071254</td>\n",
              "      <td>-0.070512</td>\n",
              "      <td>...</td>\n",
              "      <td>300.45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1035 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3         4         5         6  \\\n",
              "0 -0.057114  0.059313 -0.060191 -0.018176 -0.084338  0.034319 -0.058333   \n",
              "1 -0.074543  0.059482 -0.023707 -0.031039 -0.080793  0.020725 -0.059566   \n",
              "2 -0.088873  0.066683 -0.039078 -0.013078 -0.081851  0.002385 -0.033466   \n",
              "3 -0.079781  0.052505 -0.031692 -0.014320 -0.075479  0.012609 -0.057462   \n",
              "4 -0.067623  0.042367 -0.049939 -0.033383 -0.096456  0.037647 -0.042283   \n",
              "\n",
              "          7         8         9  ...  TOTAL DURATION (SEC)  GENDER_Male  \\\n",
              "0  0.018405  0.068168 -0.077160  ...                300.45          0.0   \n",
              "1  0.012630  0.088856 -0.075533  ...                300.45          0.0   \n",
              "2  0.038994  0.053495 -0.089920  ...                300.45          0.0   \n",
              "3 -0.004710  0.082208 -0.084615  ...                300.45          0.0   \n",
              "4  0.001679  0.071254 -0.070512  ...                300.45          0.0   \n",
              "\n",
              "   GENDER_Man  GENDER_Non-binary  GENDER_Woman  RACE/ETHNICITY_Asian  \\\n",
              "0         0.0                0.0           1.0                   1.0   \n",
              "1         0.0                0.0           1.0                   1.0   \n",
              "2         0.0                0.0           1.0                   1.0   \n",
              "3         0.0                0.0           1.0                   1.0   \n",
              "4         0.0                0.0           1.0                   1.0   \n",
              "\n",
              "   RACE/ETHNICITY_Black or African American  RACE/ETHNICITY_Hispanic/Latino  \\\n",
              "0                                       0.0                             0.0   \n",
              "1                                       0.0                             0.0   \n",
              "2                                       0.0                             0.0   \n",
              "3                                       0.0                             0.0   \n",
              "4                                       0.0                             0.0   \n",
              "\n",
              "   RACE/ETHNICITY_Two or more races  RACE/ETHNICITY_White  \n",
              "0                               0.0                   0.0  \n",
              "1                               0.0                   0.0  \n",
              "2                               0.0                   0.0  \n",
              "3                               0.0                   0.0  \n",
              "4                               0.0                   0.0  \n",
              "\n",
              "[5 rows x 1035 columns]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(embeddings_combined)\n",
        "tabular_data_reset = tabular_data.reset_index(drop=False)  # Reset index of tabular_data\n",
        "merged_features = pd.concat([df, tabular_data_reset], axis=1) # Concatenate along columns (axis=1)\n",
        "merged_features.index = merged_df.index\n",
        "merged_features = merged_features.drop(columns=['index'])\n",
        "merged_features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4Wo4V0q6AbS"
      },
      "outputs": [],
      "source": [
        "unique_pids = np.array(merged_df['PID_x'].unique())\n",
        "random_pid = np.random.choice(unique_pids, size=10, replace=False)\n",
        "# X_train, y_train = merged_features[~merged_df['PID_x'].isin(random_pid)], labels[~merged_df['PID_x'].isin(random_pid)]\n",
        "# X_test, y_test = merged_features[merged_df['PID_x'].isin(random_pid)], labels[merged_df['PID_x'].isin(random_pid)]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(merged_features, labels, test_size=0.3, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbE9fDoVOHew",
        "outputId": "54a7e4fd-1d2c-4dda-863f-f64b30b051ce"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "\n",
        "# Define the neural network architecture\n",
        "class ComplexClassifier(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(ComplexClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 256)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.3)  # Dropout for regularization\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout3 = nn.Dropout(0.3)\n",
        "        self.fc4 = nn.Linear(64, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [0],\n",
              "       [0],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = X_train.shape[1]  # Get the number of input features\n",
        "model = ComplexClassifier(input_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 2500 # Adjust as needed\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch+1) % 100 == 0:\n",
        "       print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfS4dbDvOVxE"
      },
      "outputs": [],
      "source": [
        "y_pred_tensor = model(X_test_tensor)\n",
        "y_pred = torch.argmax(y_pred_tensor, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBAZFeCaWk3K"
      },
      "outputs": [],
      "source": [
        "for pid in random_pid:\n",
        "  index = merged_df[merged_df['PID_x'] == pid].index\n",
        "  X_test_pid = merged_features.iloc[index,:]\n",
        "  X_test_scaled_pid = scaler.transform(X_test_pid)\n",
        "  X_test_tensor_pid = torch.tensor(X_test_scaled_pid, dtype=torch.float32)\n",
        "  y_pred_tensor_pid = model(X_test_tensor_pid)\n",
        "  y_pred_pid = torch.argmax(y_pred_tensor_pid, dim=1).numpy()\n",
        "  y_test_pid = labels[index]\n",
        "  acc = accuracy_score(y_test_pid, y_pred_pid)\n",
        "  auc = roc_auc_score(y_test_pid, y_pred_pid)\n",
        "  # f1 = f1_score(y_test_pid, y_pred_pid,average='micro')\n",
        "\n",
        "  # Print the results\n",
        "  print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
        "  print(f\"Test AUC: {auc * 100:.2f}%\")\n",
        "  # print(f\"Test F1 Score: {f1 * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "MkRZHmKrs4pm",
        "outputId": "fb7422b5-d390-4009-8286-2c8e081e3dc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 74.65%\n",
            "Test AUC: 73.17%\n",
            "Test F1 Score: 74.65%\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# all_preds = xgb_classifier.predict(X_test)\n",
        "all_preds = y_pred.numpy()\n",
        "# Calculate accuracy\n",
        "acc = accuracy_score(y_test, all_preds)\n",
        "auc = roc_auc_score(y_test, all_preds)\n",
        "f1_scor = f1_score(y_test, all_preds,average='micro')\n",
        "\n",
        "# Print the results\n",
        "print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
        "print(f\"Test AUC: {auc * 100:.2f}%\")\n",
        "print(f\"Test F1 Score: {f1_scor * 100:.2f}%\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, all_preds)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "#\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "#1 indicates completion of the task, 0 indicates failure\n",
        "mask_rc_lw_1 = merged_df[['RC', 'LW']] == 1\n",
        "mask_rc_lw_0 = merged_df[['RC', 'LW']] == 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "successful_completion_merged_df = merged_df[mask_rc_lw_1.all(axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_features['Pain'] = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>GENDER_Male</th>\n",
              "      <th>GENDER_Man</th>\n",
              "      <th>GENDER_Non-binary</th>\n",
              "      <th>GENDER_Woman</th>\n",
              "      <th>RACE/ETHNICITY_Asian</th>\n",
              "      <th>RACE/ETHNICITY_Black or African American</th>\n",
              "      <th>RACE/ETHNICITY_Hispanic/Latino</th>\n",
              "      <th>RACE/ETHNICITY_Two or more races</th>\n",
              "      <th>RACE/ETHNICITY_White</th>\n",
              "      <th>Pain</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.057114</td>\n",
              "      <td>0.059313</td>\n",
              "      <td>-0.060191</td>\n",
              "      <td>-0.018176</td>\n",
              "      <td>-0.084338</td>\n",
              "      <td>0.034319</td>\n",
              "      <td>-0.058333</td>\n",
              "      <td>0.018405</td>\n",
              "      <td>0.068168</td>\n",
              "      <td>-0.077160</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.074543</td>\n",
              "      <td>0.059482</td>\n",
              "      <td>-0.023707</td>\n",
              "      <td>-0.031039</td>\n",
              "      <td>-0.080793</td>\n",
              "      <td>0.020725</td>\n",
              "      <td>-0.059566</td>\n",
              "      <td>0.012630</td>\n",
              "      <td>0.088856</td>\n",
              "      <td>-0.075533</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.088873</td>\n",
              "      <td>0.066683</td>\n",
              "      <td>-0.039078</td>\n",
              "      <td>-0.013078</td>\n",
              "      <td>-0.081851</td>\n",
              "      <td>0.002385</td>\n",
              "      <td>-0.033466</td>\n",
              "      <td>0.038994</td>\n",
              "      <td>0.053495</td>\n",
              "      <td>-0.089920</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.079781</td>\n",
              "      <td>0.052505</td>\n",
              "      <td>-0.031692</td>\n",
              "      <td>-0.014320</td>\n",
              "      <td>-0.075479</td>\n",
              "      <td>0.012609</td>\n",
              "      <td>-0.057462</td>\n",
              "      <td>-0.004710</td>\n",
              "      <td>0.082208</td>\n",
              "      <td>-0.084615</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.067623</td>\n",
              "      <td>0.042367</td>\n",
              "      <td>-0.049939</td>\n",
              "      <td>-0.033383</td>\n",
              "      <td>-0.096456</td>\n",
              "      <td>0.037647</td>\n",
              "      <td>-0.042283</td>\n",
              "      <td>0.001679</td>\n",
              "      <td>0.071254</td>\n",
              "      <td>-0.070512</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7039</th>\n",
              "      <td>-0.065515</td>\n",
              "      <td>0.002374</td>\n",
              "      <td>-0.044538</td>\n",
              "      <td>0.033817</td>\n",
              "      <td>-0.140595</td>\n",
              "      <td>0.029974</td>\n",
              "      <td>-0.025950</td>\n",
              "      <td>0.034192</td>\n",
              "      <td>0.022685</td>\n",
              "      <td>-0.014089</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7040</th>\n",
              "      <td>-0.037051</td>\n",
              "      <td>0.008038</td>\n",
              "      <td>0.001874</td>\n",
              "      <td>0.033752</td>\n",
              "      <td>-0.111596</td>\n",
              "      <td>0.013145</td>\n",
              "      <td>-0.067856</td>\n",
              "      <td>-0.019096</td>\n",
              "      <td>0.024045</td>\n",
              "      <td>-0.004642</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7041</th>\n",
              "      <td>-0.055658</td>\n",
              "      <td>-0.003297</td>\n",
              "      <td>0.003345</td>\n",
              "      <td>0.001668</td>\n",
              "      <td>-0.107872</td>\n",
              "      <td>0.028781</td>\n",
              "      <td>-0.052147</td>\n",
              "      <td>-0.015808</td>\n",
              "      <td>0.047087</td>\n",
              "      <td>-0.002457</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7042</th>\n",
              "      <td>-0.068889</td>\n",
              "      <td>-0.007317</td>\n",
              "      <td>-0.020073</td>\n",
              "      <td>0.027626</td>\n",
              "      <td>-0.096740</td>\n",
              "      <td>0.030277</td>\n",
              "      <td>-0.012475</td>\n",
              "      <td>0.025860</td>\n",
              "      <td>0.028355</td>\n",
              "      <td>-0.034159</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7043</th>\n",
              "      <td>-0.064036</td>\n",
              "      <td>-0.011284</td>\n",
              "      <td>-0.004509</td>\n",
              "      <td>0.008696</td>\n",
              "      <td>-0.163027</td>\n",
              "      <td>-0.008644</td>\n",
              "      <td>0.004037</td>\n",
              "      <td>0.052301</td>\n",
              "      <td>0.057141</td>\n",
              "      <td>-0.016020</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6190 rows × 1036 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "0    -0.057114  0.059313 -0.060191 -0.018176 -0.084338  0.034319 -0.058333   \n",
              "1    -0.074543  0.059482 -0.023707 -0.031039 -0.080793  0.020725 -0.059566   \n",
              "2    -0.088873  0.066683 -0.039078 -0.013078 -0.081851  0.002385 -0.033466   \n",
              "3    -0.079781  0.052505 -0.031692 -0.014320 -0.075479  0.012609 -0.057462   \n",
              "4    -0.067623  0.042367 -0.049939 -0.033383 -0.096456  0.037647 -0.042283   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "7039 -0.065515  0.002374 -0.044538  0.033817 -0.140595  0.029974 -0.025950   \n",
              "7040 -0.037051  0.008038  0.001874  0.033752 -0.111596  0.013145 -0.067856   \n",
              "7041 -0.055658 -0.003297  0.003345  0.001668 -0.107872  0.028781 -0.052147   \n",
              "7042 -0.068889 -0.007317 -0.020073  0.027626 -0.096740  0.030277 -0.012475   \n",
              "7043 -0.064036 -0.011284 -0.004509  0.008696 -0.163027 -0.008644  0.004037   \n",
              "\n",
              "             7         8         9  ...  GENDER_Male  GENDER_Man  \\\n",
              "0     0.018405  0.068168 -0.077160  ...          0.0         0.0   \n",
              "1     0.012630  0.088856 -0.075533  ...          0.0         0.0   \n",
              "2     0.038994  0.053495 -0.089920  ...          0.0         0.0   \n",
              "3    -0.004710  0.082208 -0.084615  ...          0.0         0.0   \n",
              "4     0.001679  0.071254 -0.070512  ...          0.0         0.0   \n",
              "...        ...       ...       ...  ...          ...         ...   \n",
              "7039  0.034192  0.022685 -0.014089  ...          0.0         1.0   \n",
              "7040 -0.019096  0.024045 -0.004642  ...          0.0         1.0   \n",
              "7041 -0.015808  0.047087 -0.002457  ...          0.0         1.0   \n",
              "7042  0.025860  0.028355 -0.034159  ...          0.0         1.0   \n",
              "7043  0.052301  0.057141 -0.016020  ...          0.0         1.0   \n",
              "\n",
              "      GENDER_Non-binary  GENDER_Woman  RACE/ETHNICITY_Asian  \\\n",
              "0                   0.0           1.0                   1.0   \n",
              "1                   0.0           1.0                   1.0   \n",
              "2                   0.0           1.0                   1.0   \n",
              "3                   0.0           1.0                   1.0   \n",
              "4                   0.0           1.0                   1.0   \n",
              "...                 ...           ...                   ...   \n",
              "7039                0.0           0.0                   1.0   \n",
              "7040                0.0           0.0                   1.0   \n",
              "7041                0.0           0.0                   1.0   \n",
              "7042                0.0           0.0                   1.0   \n",
              "7043                0.0           0.0                   1.0   \n",
              "\n",
              "      RACE/ETHNICITY_Black or African American  \\\n",
              "0                                          0.0   \n",
              "1                                          0.0   \n",
              "2                                          0.0   \n",
              "3                                          0.0   \n",
              "4                                          0.0   \n",
              "...                                        ...   \n",
              "7039                                       0.0   \n",
              "7040                                       0.0   \n",
              "7041                                       0.0   \n",
              "7042                                       0.0   \n",
              "7043                                       0.0   \n",
              "\n",
              "      RACE/ETHNICITY_Hispanic/Latino  RACE/ETHNICITY_Two or more races  \\\n",
              "0                                0.0                               0.0   \n",
              "1                                0.0                               0.0   \n",
              "2                                0.0                               0.0   \n",
              "3                                0.0                               0.0   \n",
              "4                                0.0                               0.0   \n",
              "...                              ...                               ...   \n",
              "7039                             0.0                               0.0   \n",
              "7040                             0.0                               0.0   \n",
              "7041                             0.0                               0.0   \n",
              "7042                             0.0                               0.0   \n",
              "7043                             0.0                               0.0   \n",
              "\n",
              "      RACE/ETHNICITY_White  Pain  \n",
              "0                      0.0     0  \n",
              "1                      0.0     0  \n",
              "2                      0.0     0  \n",
              "3                      0.0     0  \n",
              "4                      0.0     0  \n",
              "...                    ...   ...  \n",
              "7039                   0.0     0  \n",
              "7040                   0.0     0  \n",
              "7041                   0.0     0  \n",
              "7042                   0.0     0  \n",
              "7043                   0.0     0  \n",
              "\n",
              "[6190 rows x 1036 columns]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_features[mask_rc_lw_1.all(axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = merged_features[mask_rc_lw_1.all(axis=1)].drop(columns=['Pain'])\n",
        "X.columns = X.columns.astype(str)\n",
        "Y = merged_features[mask_rc_lw_1.all(axis=1)][['Pain']].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:   4%|▍         | 106/2500 [00:03<01:07, 35.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [100/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:   8%|▊         | 203/2500 [00:06<01:09, 32.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [200/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  12%|█▏        | 303/2500 [00:09<01:08, 32.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [300/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  16%|█▌        | 404/2500 [00:12<01:02, 33.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [400/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  20%|██        | 507/2500 [00:15<00:56, 35.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [500/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  24%|██▍       | 605/2500 [00:17<00:47, 39.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [600/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  28%|██▊       | 704/2500 [00:20<00:46, 38.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [700/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  32%|███▏      | 803/2500 [00:23<00:54, 31.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [800/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  36%|███▌      | 904/2500 [00:26<00:45, 34.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [900/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  40%|████      | 1008/2500 [00:29<00:37, 40.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1000/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  44%|████▍     | 1105/2500 [00:32<00:38, 36.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1100/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  48%|████▊     | 1206/2500 [00:35<00:40, 32.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1200/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  52%|█████▏    | 1304/2500 [00:38<00:37, 32.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1300/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  56%|█████▌    | 1405/2500 [00:40<00:29, 36.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1400/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  60%|██████    | 1506/2500 [00:43<00:26, 37.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1500/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  64%|██████▍   | 1607/2500 [00:46<00:23, 38.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1600/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  68%|██████▊   | 1704/2500 [00:48<00:23, 33.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1700/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  72%|███████▏  | 1805/2500 [00:51<00:21, 32.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1800/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  76%|███████▋  | 1907/2500 [00:54<00:15, 37.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1900/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  80%|████████  | 2006/2500 [00:57<00:12, 38.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2000/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  84%|████████▍ | 2103/2500 [00:59<00:13, 29.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2100/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  88%|████████▊ | 2204/2500 [01:03<00:10, 29.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2200/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  92%|█████████▏| 2305/2500 [01:06<00:06, 32.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2300/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs:  96%|█████████▌| 2406/2500 [01:09<00:02, 35.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2400/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epochs: 100%|██████████| 2500/2500 [01:11<00:00, 34.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2500/2500], Loss: -0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = X_train.shape[1]  # Get the number of input features\n",
        "model = ComplexClassifier(input_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 2500 # Adjust as needed\n",
        "for epoch in tqdm.tqdm(range(num_epochs), desc=\"Training Epochs\"):\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch+1) % 100 == 0:\n",
        "       print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert probabilities to class labels\n",
        "np.unique(torch.argmax(outputs, dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [0],\n",
              "       [0],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "clap-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
