{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCK2qFevmaAH"
      },
      "outputs": [],
      "source": [
        "pip install laion-clap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "import laion_clap\n",
        "\n",
        "# quantization\n",
        "def int16_to_float32(x):\n",
        "    return (x / 32767.0).astype(np.float32)\n",
        "\n",
        "\n",
        "def float32_to_int16(x):\n",
        "    x = np.clip(x, a_min=-1., a_max=1.)\n",
        "    return (x * 32767.).astype(np.int16)\n",
        "\n",
        "model = laion_clap.CLAP_Module(enable_fusion=False, amodel= 'HTSAT-base')\n",
        "model.load_ckpt('/Users/ritwikvashistha/Downloads/music_speech_audioset_epoch_15_esc_89.98.pt') # download the default pretrained checkpoint.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn0RWsiqmsx3",
        "outputId": "041a609b-a299-4bf1-e24e-43d31d4e86cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/ritwikvashistha/anaconda3/envs/cv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/Users/ritwikvashistha/anaconda3/envs/cv/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3484.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/Users/ritwikvashistha/anaconda3/envs/cv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load the specified checkpoint /Users/ritwikvashistha/Downloads/music_speech_audioset_epoch_15_esc_89.98.pt from users.\n",
            "Load Checkpoint...\n",
            "logit_scale_a \t Loaded\n",
            "logit_scale_t \t Loaded\n",
            "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
            "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
            "audio_branch.logmel_extractor.melW \t Loaded\n",
            "audio_branch.bn0.weight \t Loaded\n",
            "audio_branch.bn0.bias \t Loaded\n",
            "audio_branch.patch_embed.proj.weight \t Loaded\n",
            "audio_branch.patch_embed.proj.bias \t Loaded\n",
            "audio_branch.patch_embed.norm.weight \t Loaded\n",
            "audio_branch.patch_embed.norm.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
            "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
            "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
            "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
            "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
            "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
            "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
            "audio_branch.norm.weight \t Loaded\n",
            "audio_branch.norm.bias \t Loaded\n",
            "audio_branch.tscam_conv.weight \t Loaded\n",
            "audio_branch.tscam_conv.bias \t Loaded\n",
            "audio_branch.head.weight \t Loaded\n",
            "audio_branch.head.bias \t Loaded\n",
            "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
            "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
            "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
            "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
            "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
            "text_branch.pooler.dense.weight \t Loaded\n",
            "text_branch.pooler.dense.bias \t Loaded\n",
            "text_transform.sequential.0.weight \t Loaded\n",
            "text_transform.sequential.0.bias \t Loaded\n",
            "text_transform.sequential.3.weight \t Loaded\n",
            "text_transform.sequential.3.bias \t Loaded\n",
            "text_projection.0.weight \t Loaded\n",
            "text_projection.0.bias \t Loaded\n",
            "text_projection.2.weight \t Loaded\n",
            "text_projection.2.bias \t Loaded\n",
            "audio_transform.sequential.0.weight \t Loaded\n",
            "audio_transform.sequential.0.bias \t Loaded\n",
            "audio_transform.sequential.3.weight \t Loaded\n",
            "audio_transform.sequential.3.bias \t Loaded\n",
            "audio_projection.0.weight \t Loaded\n",
            "audio_projection.0.bias \t Loaded\n",
            "audio_projection.2.weight \t Loaded\n",
            "audio_projection.2.bias \t Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from natsort import natsorted\n",
        "from scipy.io import wavfile\n",
        "\n",
        "# Path to main folder containing all subfolders\n",
        "main_folder = \"/Users/ritwikvashistha/Downloads/mic1_renamed_trim_2\"\n",
        "\n",
        "# First, get the subfolders in a natural-sorted list\n",
        "# (in case your subfolders also have numeric components in their names)\n",
        "subfolders = [\n",
        "    f for f in os.listdir(main_folder)\n",
        "    if os.path.isdir(os.path.join(main_folder, f))\n",
        "]\n",
        "subfolders = natsorted(subfolders)\n",
        "\n",
        "all_wavs = []  # to collect (filepath, sr, data) or similar\n",
        "\n",
        "for subfolder in subfolders:\n",
        "    subfolder_path = os.path.join(main_folder, subfolder)\n",
        "\n",
        "    # List .wav files in subfolder\n",
        "    wav_files = [\n",
        "        f for f in os.listdir(subfolder_path)\n",
        "        if f.lower().endswith(\".wav\")\n",
        "    ]\n",
        "    # Sort them in natural order\n",
        "    wav_files = natsorted(wav_files)\n",
        "\n",
        "    # Process each .wav file\n",
        "    for wav_file in wav_files:\n",
        "        wav_path = os.path.join(subfolder_path, wav_file)\n",
        "\n",
        "        # for example, using librosa (just as a placeholder)\n",
        "        # import librosa\n",
        "        # data, sr = librosa.load(wav_path, sr=None)\n",
        "\n",
        "        # or using scipy\n",
        "        data,sr = librosa.load(wav_path,sr=48000)\n",
        "\n",
        "        # collect or do something with the data\n",
        "        all_wavs.append((wav_path, sr, data))"
      ],
      "metadata": {
        "id": "tOTDm9ccnW-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "meta_participant = pd.read_csv('/Users/ritwikvashistha/Downloads/meta_participant.csv')\n",
        "meta_audio = pd.read_csv('/Users/ritwikvashistha/Downloads/meta_audio.csv')"
      ],
      "metadata": {
        "id": "gJdXvha5nk_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_indices = meta_audio[meta_audio['ACTION LABEL'].isin([0, 1, 2])].index\n",
        "# filtered_wavs = [all_wavs[i] for i in filtered_indices]"
      ],
      "metadata": {
        "id": "r_NMImAhnm5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get audio embeddings from audio data\n",
        "audio_data, _ = librosa.load('/Users/ritwikvashistha/Downloads/mic1_renamed_trim_2/p10085/p10085.LC.1.161.wav', sr=48000) # sample rate should be 48000\n",
        "audio_data = audio_data.reshape(1, -1) # Make it (1,T) or (N,T)\n",
        "audio_embed = model.get_audio_embedding_from_data(x = audio_data, use_tensor=False)\n",
        "print(audio_embed[:,-20:])\n",
        "print(audio_embed.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyiC3B0JmyJB",
        "outputId": "3f0c43fb-0fe4-4ceb-fe77-407bbc9dae83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.04740363  0.08318029  0.07488987  0.05759652 -0.07092518 -0.02472855\n",
            "  -0.06366072  0.0830975  -0.10711682  0.01761003 -0.02449952  0.01571384\n",
            "   0.06574909 -0.01752417 -0.0208378   0.03108453 -0.02417281 -0.01305506\n",
            "   0.05557008  0.01541279]]\n",
            "(1, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm # Import the tqdm function from the tqdm module\n",
        "\n",
        "embeddings = []\n",
        "for i in tqdm(range(len(all_wavs))): # Now, you're calling the tqdm function\n",
        "  audio_data = all_wavs[i][2]\n",
        "  audio_data = audio_data.reshape(1, -1)\n",
        "  audio_embed = model.get_audio_embedding_from_data(x=audio_data, use_tensor=False)\n",
        "  embeddings.append(audio_embed)\n",
        "\n",
        "print(embeddings[0][:,-20:])\n",
        "embeddings[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOaksYtQpNxN",
        "outputId": "129f4feb-0091-4b1e-e3b6-13ba12b2d6f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 7044/7044 [17:37<00:00,  6.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.01473507  0.01407797  0.0626817   0.08060933  0.0819184   0.01088174\n",
            "  -0.01646244 -0.03592059 -0.03780747  0.04159881 -0.02378966 -0.02779253\n",
            "  -0.09213068 -0.01054638  0.09020866 -0.01271665 -0.03600214  0.08788741\n",
            "   0.00368625 -0.05718192]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm # Import the tqdm function from the tqdm module\n",
        "\n",
        "text_embeddings = []\n",
        "for i in tqdm(range(len(meta_audio))): # Now, you're calling the tqdm function\n",
        "  text_data = meta_audio['NOTES'][i]\n",
        "  text_embed = model.get_text_embedding(text_data)\n",
        "  text_embeddings.append(text_embed)\n",
        "\n",
        "# print(text_embeddings)\n",
        "text_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBa57RwJcwJY",
        "outputId": "7cb281ab-241c-4c4f-9a46-f991ade9bc52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 7044/7044 [07:42<00:00, 15.24it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_embeddings_array = np.array(text_embeddings).squeeze()\n",
        "text_embeddings_array.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbc_aIiVg32H",
        "outputId": "34114774-584a-4b41-9bea-9d3f41d279e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7044, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('Text Embeddings.npy', text_embeddings_array)"
      ],
      "metadata": {
        "id": "Z1PBw7Z2inee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert embeddings to a NumPy array\n",
        "X = np.array(embeddings).squeeze() # Squeeze to remove extra dimensions if necessary\n",
        "X_filtered = X[filtered_indices]"
      ],
      "metadata": {
        "id": "Pc7s7rd9TtNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('Audio Embeddings.npy', X)"
      ],
      "metadata": {
        "id": "AuY-HgVWxs7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load Audio Embeddings.npy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved embeddings\n",
        "audio_embeddings = np.load('Audio Embeddings.npy')\n",
        "\n",
        "# Now you can work with the loaded embeddings\n",
        "print(audio_embeddings.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ8dsTI7cp6m",
        "outputId": "20a95ce3-22e2-4414-c740-5f7623ebc887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7044, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_combined = np.concatenate((audio_embeddings, text_embeddings_array), axis=1)\n",
        "embeddings_combined.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP8BvWs5hP0O",
        "outputId": "c0056e97-ce3b-4301-aa9c-26e49832ddde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7044, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "participant_data= meta_participant.loc[meta_participant.index.repeat(meta_participant[\"NUMBER OF FILES\"])]\n",
        "\n",
        "# 3. Reset the index (optional, but usually helpful)\n",
        "participant_data.reset_index(drop=True, inplace=True)\n",
        "# Merge meta_audio and participant_data DataFrames\n",
        "merged_df = pd.merge(meta_audio, participant_data, left_index=True, right_index=True)\n",
        "# merged_df = merged_df[merged_df['ACTION LABEL'].isin([0, 1, 2])]"
      ],
      "metadata": {
        "id": "IHHR-gJ4qdZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tabular_data = merged_df.loc[:, ['GENDER', 'AGE', 'RACE/ETHNICITY', 'TOTAL DURATION (SEC)']]\n",
        "tabular_data = pd.get_dummies(tabular_data, columns=['GENDER', 'RACE/ETHNICITY'], dtype=np.float32)\n",
        "num_tab_features = tabular_data.shape[1]"
      ],
      "metadata": {
        "id": "hXEAv_BuqocM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = merged_df.loc[:,'REVISED PAIN']\n",
        "labels= np.array([0 if x < 4 else 1 for x in labels]) # Labels: 0 or 1 for \"No Pain\"/\"Pain\""
      ],
      "metadata": {
        "id": "K_DRNesxqmCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: fit a random forest classifier using embeddings as vairables and labels as target\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "4zFTBV43sLL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(embeddings_combined)\n",
        "tabular_data_reset = tabular_data.reset_index(drop=False)  # Reset index of tabular_data\n",
        "merged_features = pd.concat([df, tabular_data_reset], axis=1) # Concatenate along columns (axis=1)\n",
        "merged_features.index = merged_df.index\n",
        "merged_features = merged_features.drop(columns=['index'])\n",
        "merged_features.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "fI6dZsYD6Ltd",
        "outputId": "5db125bb-ce21-4fec-c40c-5b376e308a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3         4         5         6  \\\n",
              "0 -0.057114  0.059313 -0.060191 -0.018176 -0.084338  0.034319 -0.058333   \n",
              "1 -0.074543  0.059482 -0.023707 -0.031039 -0.080793  0.020725 -0.059566   \n",
              "2 -0.088873  0.066683 -0.039078 -0.013078 -0.081851  0.002385 -0.033466   \n",
              "3 -0.079781  0.052505 -0.031692 -0.014320 -0.075479  0.012609 -0.057462   \n",
              "4 -0.067623  0.042367 -0.049939 -0.033383 -0.096456  0.037647 -0.042283   \n",
              "\n",
              "          7         8         9  ...  TOTAL DURATION (SEC)  GENDER_Male  \\\n",
              "0  0.018405  0.068168 -0.077160  ...                300.45          0.0   \n",
              "1  0.012630  0.088856 -0.075533  ...                300.45          0.0   \n",
              "2  0.038995  0.053495 -0.089921  ...                300.45          0.0   \n",
              "3 -0.004710  0.082208 -0.084615  ...                300.45          0.0   \n",
              "4  0.001679  0.071254 -0.070512  ...                300.45          0.0   \n",
              "\n",
              "   GENDER_Man  GENDER_Non-binary  GENDER_Woman  RACE/ETHNICITY_Asian  \\\n",
              "0         0.0                0.0           1.0                   1.0   \n",
              "1         0.0                0.0           1.0                   1.0   \n",
              "2         0.0                0.0           1.0                   1.0   \n",
              "3         0.0                0.0           1.0                   1.0   \n",
              "4         0.0                0.0           1.0                   1.0   \n",
              "\n",
              "   RACE/ETHNICITY_Black or African American  RACE/ETHNICITY_Hispanic/Latino  \\\n",
              "0                                       0.0                             0.0   \n",
              "1                                       0.0                             0.0   \n",
              "2                                       0.0                             0.0   \n",
              "3                                       0.0                             0.0   \n",
              "4                                       0.0                             0.0   \n",
              "\n",
              "   RACE/ETHNICITY_Two or more races  RACE/ETHNICITY_White  \n",
              "0                               0.0                   0.0  \n",
              "1                               0.0                   0.0  \n",
              "2                               0.0                   0.0  \n",
              "3                               0.0                   0.0  \n",
              "4                               0.0                   0.0  \n",
              "\n",
              "[5 rows x 1035 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>TOTAL DURATION (SEC)</th>\n",
              "      <th>GENDER_Male</th>\n",
              "      <th>GENDER_Man</th>\n",
              "      <th>GENDER_Non-binary</th>\n",
              "      <th>GENDER_Woman</th>\n",
              "      <th>RACE/ETHNICITY_Asian</th>\n",
              "      <th>RACE/ETHNICITY_Black or African American</th>\n",
              "      <th>RACE/ETHNICITY_Hispanic/Latino</th>\n",
              "      <th>RACE/ETHNICITY_Two or more races</th>\n",
              "      <th>RACE/ETHNICITY_White</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.057114</td>\n",
              "      <td>0.059313</td>\n",
              "      <td>-0.060191</td>\n",
              "      <td>-0.018176</td>\n",
              "      <td>-0.084338</td>\n",
              "      <td>0.034319</td>\n",
              "      <td>-0.058333</td>\n",
              "      <td>0.018405</td>\n",
              "      <td>0.068168</td>\n",
              "      <td>-0.077160</td>\n",
              "      <td>...</td>\n",
              "      <td>300.45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.074543</td>\n",
              "      <td>0.059482</td>\n",
              "      <td>-0.023707</td>\n",
              "      <td>-0.031039</td>\n",
              "      <td>-0.080793</td>\n",
              "      <td>0.020725</td>\n",
              "      <td>-0.059566</td>\n",
              "      <td>0.012630</td>\n",
              "      <td>0.088856</td>\n",
              "      <td>-0.075533</td>\n",
              "      <td>...</td>\n",
              "      <td>300.45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.088873</td>\n",
              "      <td>0.066683</td>\n",
              "      <td>-0.039078</td>\n",
              "      <td>-0.013078</td>\n",
              "      <td>-0.081851</td>\n",
              "      <td>0.002385</td>\n",
              "      <td>-0.033466</td>\n",
              "      <td>0.038995</td>\n",
              "      <td>0.053495</td>\n",
              "      <td>-0.089921</td>\n",
              "      <td>...</td>\n",
              "      <td>300.45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.079781</td>\n",
              "      <td>0.052505</td>\n",
              "      <td>-0.031692</td>\n",
              "      <td>-0.014320</td>\n",
              "      <td>-0.075479</td>\n",
              "      <td>0.012609</td>\n",
              "      <td>-0.057462</td>\n",
              "      <td>-0.004710</td>\n",
              "      <td>0.082208</td>\n",
              "      <td>-0.084615</td>\n",
              "      <td>...</td>\n",
              "      <td>300.45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.067623</td>\n",
              "      <td>0.042367</td>\n",
              "      <td>-0.049939</td>\n",
              "      <td>-0.033383</td>\n",
              "      <td>-0.096456</td>\n",
              "      <td>0.037647</td>\n",
              "      <td>-0.042283</td>\n",
              "      <td>0.001679</td>\n",
              "      <td>0.071254</td>\n",
              "      <td>-0.070512</td>\n",
              "      <td>...</td>\n",
              "      <td>300.45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1035 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_pids = np.array(merged_df['PID_x'].unique())\n",
        "random_pid = np.random.choice(unique_pids, size=10, replace=False)\n",
        "# X_train, y_train = merged_features[~merged_df['PID_x'].isin(random_pid)], labels[~merged_df['PID_x'].isin(random_pid)]\n",
        "# X_test, y_test = merged_features[merged_df['PID_x'].isin(random_pid)], labels[merged_df['PID_x'].isin(random_pid)]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(merged_features, labels, test_size=0.3, random_state=2)"
      ],
      "metadata": {
        "id": "p4Wo4V0q6AbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "\n",
        "# Define the neural network architecture\n",
        "class ComplexClassifier(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(ComplexClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 256)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.3)  # Dropout for regularization\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout3 = nn.Dropout(0.3)\n",
        "        self.fc4 = nn.Linear(64, 2)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = X_train.shape[1]  # Get the number of input features\n",
        "model = ComplexClassifier(input_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 2500 # Adjust as needed\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch+1) % 100 == 0:\n",
        "       print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbE9fDoVOHew",
        "outputId": "54a7e4fd-1d2c-4dda-863f-f64b30b051ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/ritwikvashistha/anaconda3/envs/cv/lib/python3.10/site-packages/sklearn/utils/validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/Users/ritwikvashistha/anaconda3/envs/cv/lib/python3.10/site-packages/sklearn/utils/validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n",
            "/Users/ritwikvashistha/anaconda3/envs/cv/lib/python3.10/site-packages/sklearn/utils/validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/2500], Loss: 0.1759\n",
            "Epoch [200/2500], Loss: 0.0327\n",
            "Epoch [300/2500], Loss: 0.0145\n",
            "Epoch [400/2500], Loss: 0.0203\n",
            "Epoch [500/2500], Loss: 0.0162\n",
            "Epoch [600/2500], Loss: 0.0125\n",
            "Epoch [700/2500], Loss: 0.0113\n",
            "Epoch [800/2500], Loss: 0.0111\n",
            "Epoch [900/2500], Loss: 0.0065\n",
            "Epoch [1000/2500], Loss: 0.0064\n",
            "Epoch [1100/2500], Loss: 0.0068\n",
            "Epoch [1200/2500], Loss: 0.0155\n",
            "Epoch [1300/2500], Loss: 0.0033\n",
            "Epoch [1400/2500], Loss: 0.0066\n",
            "Epoch [1500/2500], Loss: 0.0051\n",
            "Epoch [1600/2500], Loss: 0.0039\n",
            "Epoch [1700/2500], Loss: 0.0058\n",
            "Epoch [1800/2500], Loss: 0.0073\n",
            "Epoch [1900/2500], Loss: 0.0119\n",
            "Epoch [2000/2500], Loss: 0.0058\n",
            "Epoch [2100/2500], Loss: 0.0037\n",
            "Epoch [2200/2500], Loss: 0.0040\n",
            "Epoch [2300/2500], Loss: 0.0068\n",
            "Epoch [2400/2500], Loss: 0.0056\n",
            "Epoch [2500/2500], Loss: 0.0028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_tensor = model(X_test_tensor)\n",
        "y_pred = torch.argmax(y_pred_tensor, dim=1)"
      ],
      "metadata": {
        "id": "pfS4dbDvOVxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pid in random_pid:\n",
        "  index = merged_df[merged_df['PID_x'] == pid].index\n",
        "  X_test_pid = merged_features.iloc[index,:]\n",
        "  X_test_scaled_pid = scaler.transform(X_test_pid)\n",
        "  X_test_tensor_pid = torch.tensor(X_test_scaled_pid, dtype=torch.float32)\n",
        "  y_pred_tensor_pid = model(X_test_tensor_pid)\n",
        "  y_pred_pid = torch.argmax(y_pred_tensor_pid, dim=1).numpy()\n",
        "  y_test_pid = labels[index]\n",
        "  acc = accuracy_score(y_test_pid, y_pred_pid)\n",
        "  auc = roc_auc_score(y_test_pid, y_pred_pid)\n",
        "  # f1 = f1_score(y_test_pid, y_pred_pid,average='micro')\n",
        "\n",
        "  # Print the results\n",
        "  print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
        "  print(f\"Test AUC: {auc * 100:.2f}%\")\n",
        "  # print(f\"Test F1 Score: {f1 * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "EBAZFeCaWk3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# all_preds = xgb_classifier.predict(X_test)\n",
        "all_preds = y_pred.numpy()\n",
        "# Calculate accuracy\n",
        "acc = accuracy_score(y_test, all_preds)\n",
        "auc = roc_auc_score(y_test, all_preds)\n",
        "f1_score = f1_score(y_test, all_preds,average='micro')\n",
        "\n",
        "# Print the results\n",
        "print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
        "print(f\"Test AUC: {auc * 100:.2f}%\")\n",
        "print(f\"Test F1 Score: {f1_score * 100:.2f}%\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, all_preds)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "MkRZHmKrs4pm",
        "outputId": "fb7422b5-d390-4009-8286-2c8e081e3dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 74.65%\n",
            "Test AUC: 73.17%\n",
            "Test F1 Score: 74.65%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    }
  ]
}